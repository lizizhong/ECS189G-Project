nohup: ignoring input
Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
load dataset finished
************ Start ************
dataset: stage 4 text generation training dataset , method: RNN , setting: k fold cross validation , result: saver , evaluation: Four evaluate metrics: Accuracy & Precision & Recall & F1 Score
method running...
--start training...
Step: 1 Loss: 8.540590286254883
Step: 48 Loss: 6.709680557250977
Step: 95 Loss: 6.526752471923828
Step: 142 Loss: 6.47389030456543
Step: 189 Loss: 6.254371643066406
Step: 236 Loss: 6.018571376800537
Step: 283 Loss: 5.932915210723877
Step: 330 Loss: 5.570677280426025
Step: 377 Loss: 5.5118088722229
Step: 424 Loss: 5.148345470428467
Step: 471 Loss: 5.156905174255371
Step: 518 Loss: 4.881458282470703
Step: 565 Loss: 4.738059043884277
Step: 612 Loss: 4.578479290008545
Step: 659 Loss: 4.597133636474609
Step: 706 Loss: 4.420418739318848
Step: 753 Loss: 4.423529624938965
Step: 800 Loss: 4.284053802490234
Step: 847 Loss: 4.236320495605469
Step: 894 Loss: 4.134469985961914
Step: 941 Loss: 4.017765045166016
Step: 988 Loss: 4.092858791351318
Step: 1035 Loss: 3.932745933532715
Step: 1082 Loss: 3.8993983268737793
Step: 1129 Loss: 3.838046073913574
Step: 1176 Loss: 3.7986526489257812
Step: 1223 Loss: 3.705700635910034
Step: 1270 Loss: 3.6858034133911133
Step: 1317 Loss: 3.6689975261688232
Step: 1364 Loss: 3.6001362800598145
Step: 1411 Loss: 3.493133306503296
Step: 1458 Loss: 3.4252536296844482
Step: 1505 Loss: 3.5586040019989014
Step: 1552 Loss: 3.501577377319336
Step: 1599 Loss: 3.3873305320739746
Step: 1646 Loss: 3.3232593536376953
Step: 1693 Loss: 3.282747507095337
Step: 1740 Loss: 3.2764718532562256
Step: 1787 Loss: 3.3112149238586426
Step: 1834 Loss: 3.2134182453155518
Step: 1881 Loss: 3.2015609741210938
Step: 1928 Loss: 3.177582025527954
Step: 1975 Loss: 3.1147654056549072
Step: 2022 Loss: 3.0401666164398193
Step: 2069 Loss: 3.1280691623687744
Step: 2116 Loss: 3.019839286804199
Step: 2163 Loss: 3.0824122428894043
Step: 2210 Loss: 3.033651351928711
Step: 2257 Loss: 3.065762758255005
Step: 2304 Loss: 3.0207462310791016
Step: 2351 Loss: 2.9862446784973145
Step: 2398 Loss: 2.8721330165863037
Step: 2445 Loss: 2.956287384033203
Step: 2492 Loss: 2.9249346256256104
Step: 2539 Loss: 2.8606090545654297
Step: 2586 Loss: 2.885185718536377
Step: 2633 Loss: 2.9504001140594482
Step: 2680 Loss: 2.796759843826294
Step: 2727 Loss: 2.7626612186431885
Step: 2774 Loss: 2.821831464767456
Step: 2821 Loss: 2.768249750137329
Step: 2868 Loss: 2.786781072616577
Step: 2915 Loss: 2.726300001144409
Step: 2962 Loss: 2.6028554439544678
Step: 3009 Loss: 2.671024799346924
Step: 3056 Loss: 2.7312557697296143
Step: 3103 Loss: 2.732652425765991
Step: 3150 Loss: 2.616518020629883
Step: 3197 Loss: 2.7031264305114746
Step: 3244 Loss: 2.659905195236206
Step: 3291 Loss: 2.6878819465637207
Step: 3338 Loss: 2.6062779426574707
Step: 3385 Loss: 2.5306167602539062
Step: 3432 Loss: 2.6058905124664307
Step: 3479 Loss: 2.649737596511841
Step: 3526 Loss: 2.490323305130005
Step: 3573 Loss: 2.5949771404266357
Step: 3620 Loss: 2.5341265201568604
Step: 3667 Loss: 2.6255509853363037
Step: 3714 Loss: 2.571665048599243
Step: 3761 Loss: 2.45640230178833
Step: 3808 Loss: 2.533806800842285
Step: 3855 Loss: 2.4657175540924072
Step: 3902 Loss: 2.5475716590881348
Step: 3949 Loss: 2.47515606880188
Step: 3996 Loss: 2.4605307579040527
Step: 4043 Loss: 2.4671247005462646
Step: 4090 Loss: 2.428466796875
Step: 4137 Loss: 2.5141072273254395
Step: 4184 Loss: 2.3674724102020264
Step: 4231 Loss: 2.4115045070648193
Step: 4278 Loss: 2.35813307762146
Step: 4325 Loss: 2.4513494968414307
Step: 4372 Loss: 2.3202240467071533
Step: 4419 Loss: 2.37921404838562
Step: 4466 Loss: 2.3793604373931885
Step: 4513 Loss: 2.2965941429138184
Step: 4560 Loss: 2.3907558917999268
Step: 4607 Loss: 2.32700777053833
Step: 4654 Loss: 2.3510329723358154
Step: 4701 Loss: 2.393754005432129
Step: 4748 Loss: 2.361215114593506
Step: 4795 Loss: 2.4141576290130615
Step: 4842 Loss: 2.359572649002075
Step: 4889 Loss: 2.3902475833892822
Step: 4936 Loss: 2.2727177143096924
Step: 4983 Loss: 2.3609120845794678
Step: 5030 Loss: 2.3859755992889404
Step: 5077 Loss: 2.2088990211486816
Step: 5124 Loss: 2.234757423400879
Step: 5171 Loss: 2.324435234069824
Step: 5218 Loss: 2.2933835983276367
Step: 5265 Loss: 2.29006028175354
Step: 5312 Loss: 2.2745211124420166
Step: 5359 Loss: 2.277724027633667
Step: 5406 Loss: 2.282560110092163
Step: 5453 Loss: 2.2807161808013916
Step: 5500 Loss: 2.1968727111816406
Step: 5547 Loss: 2.1482229232788086
Step: 5594 Loss: 2.2359206676483154
Step: 5641 Loss: 2.1989240646362305
Step: 5688 Loss: 2.214064359664917
Step: 5735 Loss: 2.2572407722473145
Step: 5782 Loss: 2.151487112045288
Step: 5829 Loss: 2.138742446899414
Step: 5876 Loss: 2.0941309928894043
Step: 5923 Loss: 2.1762335300445557
Step: 5970 Loss: 2.2437007427215576
Step: 6017 Loss: 2.1180717945098877
Step: 6064 Loss: 2.3245296478271484
Step: 6111 Loss: 2.2942142486572266
Step: 6158 Loss: 2.203589677810669
Step: 6205 Loss: 2.2155721187591553
Step: 6252 Loss: 2.146153450012207
Step: 6299 Loss: 2.190688133239746
Step: 6346 Loss: 2.068753480911255
Step: 6393 Loss: 2.1837666034698486
Step: 6440 Loss: 2.1452479362487793
Step: 6487 Loss: 2.140076160430908
Step: 6534 Loss: 2.0944933891296387
Step: 6581 Loss: 2.1193158626556396
Step: 6628 Loss: 2.0667245388031006
Step: 6675 Loss: 2.223909378051758
Step: 6722 Loss: 2.106670379638672
Step: 6769 Loss: 2.1627871990203857
Step: 6816 Loss: 2.1429338455200195
Step: 6863 Loss: 2.0543558597564697
Step: 6910 Loss: 2.065274953842163
Step: 6957 Loss: 2.118389844894409
Step: 7004 Loss: 2.2288529872894287
Step: 7051 Loss: 2.0801162719726562
Step: 7098 Loss: 2.1623103618621826
Step: 7145 Loss: 2.0525753498077393
Step: 7192 Loss: 2.1226694583892822
Step: 7239 Loss: 2.0885114669799805
Step: 7286 Loss: 2.084829330444336
Step: 7333 Loss: 2.085726261138916
Step: 7380 Loss: 2.0533359050750732
Step: 7427 Loss: 2.0220580101013184
Step: 7474 Loss: 2.0469448566436768
Step: 7521 Loss: 2.0561089515686035
Step: 7568 Loss: 2.0015487670898438
Step: 7615 Loss: 2.021080255508423
Step: 7662 Loss: 1.9918957948684692
Step: 7709 Loss: 2.0936360359191895
Step: 7756 Loss: 2.0102005004882812
Step: 7803 Loss: 2.033904552459717
Step: 7850 Loss: 1.9956886768341064
Step: 7897 Loss: 1.9612641334533691
Step: 7944 Loss: 2.0744879245758057
Step: 7991 Loss: 2.0100784301757812
Step: 8038 Loss: 2.074536085128784
Step: 8085 Loss: 1.9698748588562012
Step: 8132 Loss: 2.0289320945739746
Step: 8179 Loss: 2.091252326965332
Step: 8226 Loss: 2.053133964538574
Step: 8273 Loss: 2.026496410369873
Step: 8320 Loss: 1.9449049234390259
Step: 8367 Loss: 2.001282215118408
Step: 8414 Loss: 2.0385050773620605
Step: 8461 Loss: 1.9473930597305298
Step: 8508 Loss: 2.038733959197998
Step: 8555 Loss: 1.9485756158828735
Step: 8602 Loss: 1.9645575284957886
Step: 8649 Loss: 1.9709298610687256
Step: 8696 Loss: 1.9269980192184448
Step: 8743 Loss: 2.0411646366119385
Step: 8790 Loss: 1.952975869178772
Step: 8837 Loss: 1.9543145895004272
Step: 8884 Loss: 1.9832351207733154
Step: 8931 Loss: 2.0090534687042236
Step: 8978 Loss: 1.9928641319274902
Step: 9025 Loss: 1.9009904861450195
Step: 9072 Loss: 2.0181374549865723
Step: 9119 Loss: 1.924524188041687
Step: 9166 Loss: 1.9235429763793945
Step: 9213 Loss: 1.942733883857727
Step: 9260 Loss: 2.0192203521728516
Step: 9307 Loss: 1.944744348526001
Step: 9354 Loss: 1.9132212400436401
Step: 9401 Loss: 1.9033079147338867
Step: 9448 Loss: 2.029881000518799
Step: 9495 Loss: 1.9399659633636475
Step: 9542 Loss: 1.9897717237472534
Step: 9589 Loss: 1.9401311874389648
Step: 9636 Loss: 1.9300193786621094
Step: 9683 Loss: 1.978368878364563
Step: 9730 Loss: 1.9011304378509521
Step: 9777 Loss: 1.9099284410476685
Step: 9824 Loss: 1.925302505493164
Step: 9871 Loss: 1.8814324140548706
Step: 9918 Loss: 1.9412232637405396
Step: 9965 Loss: 1.9397001266479492
Step: 10012 Loss: 1.9204018115997314
Step: 10059 Loss: 1.9081499576568604
Step: 10106 Loss: 1.8722227811813354
Step: 10153 Loss: 1.9131724834442139
Step: 10200 Loss: 1.9627161026000977
Step: 10247 Loss: 1.9040268659591675
Step: 10294 Loss: 1.871463656425476
Step: 10341 Loss: 1.8745286464691162
Step: 10388 Loss: 1.9563641548156738
Step: 10435 Loss: 