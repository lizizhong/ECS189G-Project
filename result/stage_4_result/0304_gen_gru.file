nohup: ignoring input
Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
load dataset finished
************ Start ************
dataset: stage 4 text generation training dataset , method: RNN , setting: k fold cross validation , result: saver , evaluation: Four evaluate metrics: Accuracy & Precision & Recall & F1 Score
method running...
--start training...
Step: 1 Loss: 8.50124454498291
Step: 48 Loss: 6.721168041229248
Step: 95 Loss: 6.586546421051025
Step: 142 Loss: 6.524733066558838
Step: 189 Loss: 6.421352386474609
Step: 236 Loss: 6.267627239227295
Step: 283 Loss: 6.057847499847412
Step: 330 Loss: 5.940655708312988
Step: 377 Loss: 5.777184009552002
Step: 424 Loss: 5.507999420166016
Step: 471 Loss: 5.400328636169434
Step: 518 Loss: 5.406627655029297
Step: 565 Loss: 5.255232334136963
Step: 612 Loss: 5.004298686981201
Step: 659 Loss: 4.9729228019714355
Step: 706 Loss: 4.82431697845459
Step: 753 Loss: 4.705333709716797
Step: 800 Loss: 4.6929521560668945
Step: 847 Loss: 4.489951133728027
Step: 894 Loss: 4.554380416870117
Step: 941 Loss: 4.293972969055176
Step: 988 Loss: 4.249192237854004
Step: 1035 Loss: 4.2265424728393555
Step: 1082 Loss: 4.15419340133667
Step: 1129 Loss: 4.131541728973389
Step: 1176 Loss: 3.9972498416900635
Step: 1223 Loss: 3.829113006591797
Step: 1270 Loss: 3.8780834674835205
Step: 1317 Loss: 3.8834903240203857
Step: 1364 Loss: 3.811673879623413
Step: 1411 Loss: 3.732797622680664
Step: 1458 Loss: 3.6736485958099365
Step: 1505 Loss: 3.7833337783813477
Step: 1552 Loss: 3.726256847381592
Step: 1599 Loss: 3.632235527038574
Step: 1646 Loss: 3.518281936645508
Step: 1693 Loss: 3.437607765197754
Step: 1740 Loss: 3.5328071117401123
Step: 1787 Loss: 3.405590534210205
Step: 1834 Loss: 3.44229793548584
Step: 1881 Loss: 3.2074365615844727
Step: 1928 Loss: 3.295461654663086
Step: 1975 Loss: 3.227715253829956
Step: 2022 Loss: 3.3014488220214844
Step: 2069 Loss: 3.233517646789551
Step: 2116 Loss: 3.1592869758605957
Step: 2163 Loss: 3.216832160949707
Step: 2210 Loss: 3.0809085369110107
Step: 2257 Loss: 3.148185968399048
Step: 2304 Loss: 3.088919162750244
Step: 2351 Loss: 3.1318604946136475
Step: 2398 Loss: 3.0177297592163086
Step: 2445 Loss: 3.00925612449646
Step: 2492 Loss: 2.9308462142944336
Step: 2539 Loss: 2.9959216117858887
Step: 2586 Loss: 2.861363172531128
Step: 2633 Loss: 2.9720873832702637
Step: 2680 Loss: 2.829003095626831
Step: 2727 Loss: 2.9618258476257324
Step: 2774 Loss: 2.983802556991577
Step: 2821 Loss: 2.771436929702759
Step: 2868 Loss: 2.786036491394043
Step: 2915 Loss: 2.824164390563965
Step: 2962 Loss: 2.807448387145996
Step: 3009 Loss: 2.7133119106292725
Step: 3056 Loss: 2.678358554840088
Step: 3103 Loss: 2.6031298637390137
Step: 3150 Loss: 2.6676783561706543
Step: 3197 Loss: 2.755171060562134
Step: 3244 Loss: 2.6119303703308105
Step: 3291 Loss: 2.6005423069000244
Step: 3338 Loss: 2.6295714378356934
Step: 3385 Loss: 2.579188346862793
Step: 3432 Loss: 2.698793411254883
Step: 3479 Loss: 2.656210422515869
Step: 3526 Loss: 2.538957118988037
Step: 3573 Loss: 2.4913320541381836
Step: 3620 Loss: 2.486717462539673
Step: 3667 Loss: 2.5648787021636963
Step: 3714 Loss: 2.5831029415130615
Step: 3761 Loss: 2.5422651767730713
Step: 3808 Loss: 2.5549519062042236
Step: 3855 Loss: 2.5603628158569336
Step: 3902 Loss: 2.520488977432251
Step: 3949 Loss: 2.4670398235321045
Step: 3996 Loss: 2.5083467960357666
Step: 4043 Loss: 2.461768388748169
Step: 4090 Loss: 2.4512603282928467
Step: 4137 Loss: 2.374880313873291
Step: 4184 Loss: 2.444020986557007
Step: 4231 Loss: 2.4553823471069336
Step: 4278 Loss: 2.463266611099243
Step: 4325 Loss: 2.410747766494751
Step: 4372 Loss: 2.2939181327819824
Step: 4419 Loss: 2.4037811756134033
Step: 4466 Loss: 2.4050354957580566
Step: 4513 Loss: 2.44807505607605
Step: 4560 Loss: 2.423726797103882
Step: 4607 Loss: 2.3114547729492188
Step: 4654 Loss: 2.2672805786132812
Step: 4701 Loss: 2.227872371673584
Step: 4748 Loss: 2.373070478439331
Step: 4795 Loss: 2.361581563949585
Step: 4842 Loss: 2.2835865020751953
Step: 4889 Loss: 2.300649642944336
Step: 4936 Loss: 2.2908105850219727
Step: 4983 Loss: 2.2657413482666016
Step: 5030 Loss: 2.336907386779785
Step: 5077 Loss: 2.2292001247406006
Step: 5124 Loss: 2.2661969661712646
Step: 5171 Loss: 2.1924850940704346
Step: 5218 Loss: 2.2318954467773438
Step: 5265 Loss: 2.204345703125
Step: 5312 Loss: 2.333362340927124
Step: 5359 Loss: 2.2070932388305664
Step: 5406 Loss: 2.1841275691986084
Step: 5453 Loss: 2.130786418914795
Step: 5500 Loss: 2.209056854248047
Step: 5547 Loss: 2.174137830734253
Step: 5594 Loss: 2.083977460861206
Step: 5641 Loss: 2.2013328075408936
Step: 5688 Loss: 2.148073196411133
Step: 5735 Loss: 2.0585947036743164
Step: 5782 Loss: 2.2239418029785156
Step: 5829 Loss: 2.093757152557373
Step: 5876 Loss: 2.154202461242676
Step: 5923 Loss: 2.1334054470062256
Step: 5970 Loss: 2.1582233905792236
Step: 6017 Loss: 2.1506659984588623
Step: 6064 Loss: 2.168917655944824
Step: 6111 Loss: 2.0927817821502686
Step: 6158 Loss: 2.0870754718780518
Step: 6205 Loss: 2.0840418338775635
Step: 6252 Loss: 2.1147279739379883
Step: 6299 Loss: 2.162461519241333
Step: 6346 Loss: 2.0381274223327637
Step: 6393 Loss: 2.102600574493408
Step: 6440 Loss: 2.077892303466797
Step: 6487 Loss: 2.1261003017425537
Step: 6534 Loss: 2.067030191421509
Step: 6581 Loss: 2.129831552505493
Step: 6628 Loss: 2.0453948974609375
Step: 6675 Loss: 2.1236796379089355
Step: 6722 Loss: 2.0055510997772217
Step: 6769 Loss: 2.1092002391815186
Step: 6816 Loss: 2.1205484867095947
Step: 6863 Loss: 2.10444974899292
Step: 6910 Loss: 2.0602667331695557
Step: 6957 Loss: 1.9969756603240967
Step: 7004 Loss: 1.976642370223999
Step: 7051 Loss: 2.011756420135498
Step: 7098 Loss: 2.0366034507751465
Step: 7145 Loss: 2.021519660949707
Step: 7192 Loss: 2.0156445503234863
Step: 7239 Loss: 1.982895016670227
Step: 7286 Loss: 2.027026891708374
Step: 7333 Loss: 1.9887436628341675
Step: 7380 Loss: 1.9582030773162842
Step: 7427 Loss: 2.076267719268799
Step: 7474 Loss: 1.9806097745895386
Step: 7521 Loss: 1.9837472438812256
Step: 7568 Loss: 1.9982421398162842
Step: 7615 Loss: 1.977414846420288
Step: 7662 Loss: 1.9953250885009766
Step: 7709 Loss: 2.009087562561035
Step: 7756 Loss: 1.9236695766448975
Step: 7803 Loss: 1.873711347579956
Step: 7850 Loss: 1.9990990161895752
Step: 7897 Loss: 1.8736941814422607
Step: 7944 Loss: 1.9879436492919922
Step: 7991 Loss: 1.955329418182373
Step: 8038 Loss: 1.9908820390701294
Step: 8085 Loss: 1.9860035181045532
Step: 8132 Loss: 1.9323453903198242
Step: 8179 Loss: 2.0172157287597656
Step: 8226 Loss: 1.9009990692138672
Step: 8273 Loss: 1.9609929323196411
Step: 8320 Loss: 1.9108569622039795
Step: 8367 Loss: 1.8657971620559692
Step: 8414 Loss: 1.8327195644378662
Step: 8461 Loss: 1.9209176301956177
Step: 8508 Loss: 1.9898061752319336
Step: 8555 Loss: 1.852101445198059
Step: 8602 Loss: 1.9321650266647339
Step: 8649 Loss: 1.9490958452224731
Step: 8696 Loss: 1.895651936531067
Step: 8743 Loss: 1.9325803518295288
Step: 8790 Loss: 1.8873703479766846
Step: 8837 Loss: 1.91146981716156
Step: 8884 Loss: 2.0003488063812256
Step: 8931 Loss: 1.8690062761306763
Step: 8978 Loss: 1.9584276676177979
Step: 9025 Loss: 1.8999520540237427
Step: 9072 Loss: 1.919631838798523
Step: 9119 Loss: 1.865979790687561
Step: 9166 Loss: 1.81498122215271
Step: 9213 Loss: 1.9187991619110107
Step: 9260 Loss: 1.9149999618530273
Step: 9307 Loss: 1.9154753684997559
Step: 9354 Loss: 1.8047668933868408
Step: 9401 Loss: 1.8971352577209473
Step: 9448 Loss: 1.805228352546692
Step: 9495 Loss: 1.8693108558654785
Step: 9542 Loss: 1.8808320760726929
Step: 9589 Loss: 1.8564990758895874
Step: 9636 Loss: 1.8757041692733765
Step: 9683 Loss: 1.8615182638168335
Step: 9730 Loss: 1.8938720226287842
Step: 9777 Loss: 1.8498592376708984
Step: 9824 Loss: 1.7933038473129272
Step: 9871 Loss: 1.8167649507522583
Step: 9918 Loss: 1.8665809631347656
Step: 9965 Loss: 1.8653500080108643
Step: 10012 Loss: 1.813748836517334
Step: 10059 Loss: 1.8280490636825562
Step: 10106 Loss: 1.8530441522598267
Step: 10153 Loss: 1.7581273317337036
Step: 10200 Loss: 1.8465665578842163
Step: 10247 Loss: 1.8019956350326538
Step: 10294 Loss: 1.768412470817566
Step: 10341 Loss: 1.8498692512512207
Step: 10388 Loss: 1.8560044765472412
Step: 10435 Loss: 1.8339215517044067
Step: 