nohup: ignoring input
Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
load dataset finished
************ Start ************
dataset: stage 4 text generation training dataset , method: RNN , setting: k fold cross validation , result: saver , evaluation: Four evaluate metrics: Accuracy & Precision & Recall & F1 Score
method running...
--start training...
Step: 1 Loss: 8.502922058105469
Step: 48 Loss: 6.676414489746094
Step: 95 Loss: 6.548208236694336
Step: 142 Loss: 6.628492832183838
Step: 189 Loss: 6.571493148803711
Step: 236 Loss: 6.639510631561279
Step: 283 Loss: 6.543086528778076
Step: 330 Loss: 6.432392120361328
Step: 377 Loss: 6.408965110778809
Step: 424 Loss: 6.22514533996582
Step: 471 Loss: 6.147800922393799
Step: 518 Loss: 6.116825103759766
Step: 565 Loss: 6.081622123718262
Step: 612 Loss: 6.08707332611084
Step: 659 Loss: 6.010537624359131
Step: 706 Loss: 5.991055965423584
Step: 753 Loss: 5.803152084350586
Step: 800 Loss: 5.763466835021973
Step: 847 Loss: 5.6897478103637695
Step: 894 Loss: 5.697358131408691
Step: 941 Loss: 5.56631326675415
Step: 988 Loss: 5.4542927742004395
Step: 1035 Loss: 5.420178413391113
Step: 1082 Loss: 5.137861251831055
Step: 1129 Loss: 5.3489837646484375
Step: 1176 Loss: 5.2237396240234375
Step: 1223 Loss: 5.296456336975098
Step: 1270 Loss: 5.137797832489014
Step: 1317 Loss: 5.092492580413818
Step: 1364 Loss: 4.944406032562256
Step: 1411 Loss: 5.048688888549805
Step: 1458 Loss: 5.015933513641357
Step: 1505 Loss: 4.977969646453857
Step: 1552 Loss: 5.024067401885986
Step: 1599 Loss: 4.7725043296813965
Step: 1646 Loss: 4.816110610961914
Step: 1693 Loss: 4.690899848937988
Step: 1740 Loss: 4.726889610290527
Step: 1787 Loss: 4.743898868560791
Step: 1834 Loss: 4.768931865692139
Step: 1881 Loss: 4.685929775238037
Step: 1928 Loss: 4.646365642547607
Step: 1975 Loss: 4.621698379516602
Step: 2022 Loss: 4.56385612487793
Step: 2069 Loss: 4.466238498687744
Step: 2116 Loss: 4.501301288604736
Step: 2163 Loss: 4.384889602661133
Step: 2210 Loss: 4.439138889312744
Step: 2257 Loss: 4.370936393737793
Step: 2304 Loss: 4.424139976501465
Step: 2351 Loss: 4.267165184020996
Step: 2398 Loss: 4.2530412673950195
Step: 2445 Loss: 4.2648186683654785
Step: 2492 Loss: 4.212028503417969
Step: 2539 Loss: 4.2193427085876465
Step: 2586 Loss: 4.177304744720459
Step: 2633 Loss: 4.307500839233398
Step: 2680 Loss: 4.1060686111450195
Step: 2727 Loss: 4.152914524078369
Step: 2774 Loss: 4.170561790466309
Step: 2821 Loss: 4.055721282958984
Step: 2868 Loss: 4.059170722961426
Step: 2915 Loss: 4.123263835906982
Step: 2962 Loss: 4.088338851928711
Step: 3009 Loss: 3.9921388626098633
Step: 3056 Loss: 4.09345006942749
Step: 3103 Loss: 4.028884410858154
Step: 3150 Loss: 4.00135612487793
Step: 3197 Loss: 3.9012060165405273
Step: 3244 Loss: 4.051888942718506
Step: 3291 Loss: 3.9884674549102783
Step: 3338 Loss: 3.835972309112549
Step: 3385 Loss: 3.7940025329589844
Step: 3432 Loss: 3.7939705848693848
Step: 3479 Loss: 3.8909835815429688
Step: 3526 Loss: 3.6563518047332764
Step: 3573 Loss: 3.731834650039673
Step: 3620 Loss: 3.730621576309204
Step: 3667 Loss: 3.787184238433838
Step: 3714 Loss: 3.76141619682312
Step: 3761 Loss: 3.5845518112182617
Step: 3808 Loss: 3.667759895324707
Step: 3855 Loss: 3.6771867275238037
Step: 3902 Loss: 3.6523523330688477
Step: 3949 Loss: 3.5983104705810547
Step: 3996 Loss: 3.6891250610351562
Step: 4043 Loss: 3.6010680198669434
Step: 4090 Loss: 3.6864352226257324
Step: 4137 Loss: 3.537553310394287
Step: 4184 Loss: 3.606374979019165
Step: 4231 Loss: 3.4935107231140137
Step: 4278 Loss: 3.518113374710083
Step: 4325 Loss: 3.570040464401245
Step: 4372 Loss: 3.3906641006469727
Step: 4419 Loss: 3.383040428161621
Step: 4466 Loss: 3.5994529724121094
Step: 4513 Loss: 3.434993028640747
Step: 4560 Loss: 3.5475776195526123
Step: 4607 Loss: 3.514265298843384
Step: 4654 Loss: 3.459174394607544
Step: 4701 Loss: 3.469050407409668
Step: 4748 Loss: 3.3796956539154053
Step: 4795 Loss: 3.4255223274230957
Step: 4842 Loss: 3.438741683959961
Step: 4889 Loss: 3.3347361087799072
Step: 4936 Loss: 3.442230463027954
Step: 4983 Loss: 3.3649516105651855
Step: 5030 Loss: 3.372365713119507
Step: 5077 Loss: 3.368995428085327
Step: 5124 Loss: 3.28928804397583
Step: 5171 Loss: 3.270099639892578
Step: 5218 Loss: 3.212069034576416
Step: 5265 Loss: 3.3132688999176025
Step: 5312 Loss: 3.320009708404541
Step: 5359 Loss: 3.3447368144989014
Step: 5406 Loss: 3.204833984375
Step: 5453 Loss: 3.3176636695861816
Step: 5500 Loss: 3.2573304176330566
Step: 5547 Loss: 3.183171033859253
Step: 5594 Loss: 3.140029191970825
Step: 5641 Loss: 3.2442147731781006
Step: 5688 Loss: 3.176462173461914
Step: 5735 Loss: 3.222372055053711
Step: 5782 Loss: 3.122112512588501
Step: 5829 Loss: 3.058289051055908
Step: 5876 Loss: 3.1440889835357666
Step: 5923 Loss: 3.175428628921509
Step: 5970 Loss: 3.1969382762908936
Step: 6017 Loss: 3.1656532287597656
Step: 6064 Loss: 3.152595281600952
Step: 6111 Loss: 3.1586861610412598
Step: 6158 Loss: 3.09249210357666
Step: 6205 Loss: 3.2436561584472656
Step: 6252 Loss: 3.071922540664673
Step: 6299 Loss: 3.1421585083007812
Step: 6346 Loss: 3.0855283737182617
Step: 6393 Loss: 3.077322006225586
Step: 6440 Loss: 2.9811649322509766
Step: 6487 Loss: 3.0528738498687744
Step: 6534 Loss: 3.034294366836548
Step: 6581 Loss: 3.075660228729248
Step: 6628 Loss: 2.978067636489868
Step: 6675 Loss: 2.994790554046631
Step: 6722 Loss: 3.022752285003662
Step: 6769 Loss: 3.0664405822753906
Step: 6816 Loss: 3.071143865585327
Step: 6863 Loss: 3.077021598815918
Step: 6910 Loss: 3.001725196838379
Step: 6957 Loss: 2.955579996109009
Step: 7004 Loss: 2.9151923656463623
Step: 7051 Loss: 3.028265953063965
Step: 7098 Loss: 3.004812717437744
Step: 7145 Loss: 3.0005621910095215
Step: 7192 Loss: 3.0373964309692383
Step: 7239 Loss: 3.0126729011535645
Step: 7286 Loss: 2.977938413619995
Step: 7333 Loss: 3.038908004760742
Step: 7380 Loss: 2.944730043411255
Step: 7427 Loss: 2.906724691390991
Step: 7474 Loss: 2.931858539581299
Step: 7521 Loss: 2.9429824352264404
Step: 7568 Loss: 2.9180543422698975
Step: 7615 Loss: 2.9155611991882324
Step: 7662 Loss: 2.8102004528045654
Step: 7709 Loss: 2.8180294036865234
Step: 7756 Loss: 2.921060800552368
Step: 7803 Loss: 2.9243099689483643
Step: 7850 Loss: 2.8086180686950684
Step: 7897 Loss: 2.9119341373443604
Step: 7944 Loss: 2.7844860553741455
Step: 7991 Loss: 2.802271842956543
Step: 8038 Loss: 2.785200357437134
Step: 8085 Loss: 2.8975441455841064
Step: 8132 Loss: 2.7253992557525635
Step: 8179 Loss: 2.734144926071167
Step: 8226 Loss: 2.7981295585632324
Step: 8273 Loss: 2.8572239875793457
Step: 8320 Loss: 2.8196589946746826
Step: 8367 Loss: 2.8436660766601562
Step: 8414 Loss: 2.8672657012939453
Step: 8461 Loss: 2.804750680923462
Step: 8508 Loss: 2.8111159801483154
Step: 8555 Loss: 2.8659164905548096
Step: 8602 Loss: 2.7620303630828857
Step: 8649 Loss: 2.8033623695373535
Step: 8696 Loss: 2.821765422821045
Step: 8743 Loss: 2.7747840881347656
Step: 8790 Loss: 2.8463454246520996
Step: 8837 Loss: 2.794288158416748
Step: 8884 Loss: 2.7402186393737793
Step: 8931 Loss: 2.73673939704895
Step: 8978 Loss: 2.734626293182373
Step: 9025 Loss: 2.693605661392212
Step: 9072 Loss: 2.7322168350219727
Step: 9119 Loss: 2.757108688354492
Step: 9166 Loss: 2.7081217765808105
Step: 9213 Loss: 2.820326328277588
Step: 9260 Loss: 2.712677478790283
Step: 9307 Loss: 2.762263298034668
Step: 9354 Loss: 2.7357773780822754
Step: 9401 Loss: 2.6642115116119385
Step: 9448 Loss: 2.74184250831604
Step: 9495 Loss: 2.5994515419006348
Step: 9542 Loss: 2.6574060916900635
Step: 9589 Loss: 2.6793601512908936
Step: 9636 Loss: 2.6428427696228027
Step: 9683 Loss: 2.6473965644836426
Step: 9730 Loss: 2.7715365886688232
Step: 9777 Loss: 2.695533275604248
Step: 9824 Loss: 2.7162396907806396
Step: 9871 Loss: 2.640667676925659
Step: 9918 Loss: 2.620889186859131
Step: 9965 Loss: 2.607558012008667
Step: 10012 Loss: 2.61257004737854
Step: 10059 Loss: 2.6730382442474365
Step: 10106 Loss: 2.5685412883758545
Step: 10153 Loss: 2.62044095993042
Step: 10200 Loss: 2.6320719718933105
Step: 10247 Loss: 2.585167169570923
Step: 10294 Loss: 2.585983991622925
Step: 10341 Loss: 2.6552722454071045
Step: 10388 Loss: 2.662767171859741
Step: 10435 Loss: 2.609778881072998
Step: 10482 Loss: 2.569338321685791
Step: 